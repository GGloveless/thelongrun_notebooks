{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "data, target = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial regularization in linear models\n",
    "This example shows how to perform regularization in **linear models** only for a subset of variables. This short tutorial is based on the reply of a question posted in StackOverflow: https://stats.stackexchange.com/a/307133\n",
    " \n",
    "For this example, the `Diabetes dataset` will be used. A detailed description of thos dataset can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes)\n",
    "\n",
    "The first model to be tested will be a standard Lasso, for which the following equation holds:\n",
    "$$ E = ||\\mathbf{y} - \\mathbf{X}_1 \\boldsymbol{\\beta}_1 - \\mathbf{X}_2 \\boldsymbol{\\beta}_2||^2 + \\lambda ||\\boldsymbol{\\beta}||_1$$\n",
    "\n",
    "For simplicity, this equation takes into account just two independent variables. As you notice, regularization is applied equally to both variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "53.56340262454339"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_full = Lasso(alpha=0.01)\n",
    "model_full.fit(data, target)\n",
    "target_pred = model_full.predict(data)\n",
    "# Show the RMSE\n",
    "mean_squared_error(target, target_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second case will be partial regularization. This may be handy when dealing with multiple variables and you are pretty sure there are some of them have to be included. Taken directly from the StackOverflow post, I'll show you the equation: \n",
    "$$ E = ||\\mathbf{y} - \\mathbf{X}_1 \\boldsymbol{\\beta}_1 - \\mathbf{X}_2 \\boldsymbol{\\beta}_2||^2 + \\lambda ||\\boldsymbol{\\beta}_1||_1$$\n",
    "In our dataset, we choose the blood serum measurements (the last six variables) as $X_1$ and their corresponding coefficients $\\beta_1$ are the ones that will be regularized.\n",
    "Despite not being explicitly explained how to do that with Scikit-Learn, everything comes down to the following equation: \n",
    "$$\n",
    "\\begin{align*}\n",
    " \\hat\\beta_1 \n",
    " & = \\arg\\min_{\\beta_1} \\left\\{ 0 + \\|\\left(I-H_2\\right)\\left(y - X_1\\beta_1 \\right) \\|_2^2 + \\lambda \\|\\beta_1 \\|_1 \\right\\} \\\\\n",
    " & =\\arg\\min_{\\beta_1} \\left\\{ \\|\\left(I-H_2\\right)y - \\left(I-H_2\\right)X_1\\beta_1 \\|_2^2 + \\lambda \\|\\beta_1 \\|_1 \\right\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "What you see here is the expression of the loss function where data has been projected onto the complementary space of $H_2$. This space has the peculiarity of not being explainable by $\\beta_2$. Actually is independent from it according to all the assumptions made to get to it. In order to arrive to a compatible formula to use `Lasso` object we just need to make the following substitutions.\n",
    "$$\n",
    "y^{\\prime} = \\left(I-H_2\\right)y \\qquad X^{\\prime}_1 = \\left(I-H_2\\right)X_1\n",
    "$$\n",
    "To eventually get:\n",
    "$$\n",
    "\\hat\\beta_1 = \\arg\\min_{\\beta_1} \\left\\{ \\|y^{\\prime} - X^{\\prime}_1\\beta_1 \\|_2^2 + \\lambda \\|\\beta_1 \\|_1 \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.00761648, -0.00579619,  0.00587843, ..., -0.00011387,\n         0.00428299, -0.0123322 ],\n       [-0.00579619,  0.0182867 , -0.00221394, ..., -0.00270669,\n        -0.00196578,  0.01678911],\n       [ 0.00587843, -0.00221394,  0.00529319, ..., -0.00026381,\n         0.00312404, -0.00744863],\n       ...,\n       [-0.00011387, -0.00270669, -0.00026381, ...,  0.00695184,\n        -0.0037751 , -0.01224746],\n       [ 0.00428299, -0.00196578,  0.00312404, ..., -0.0037751 ,\n         0.0058978 , -0.00420768],\n       [-0.0123322 ,  0.01678911, -0.00744863, ..., -0.01224746,\n        -0.00420768,  0.05676557]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xreg = data[:,:-6] # X1\n",
    "Xnot = data[:,-6:] # X2\n",
    "# This will be our H2\n",
    "Hnot = Xnot@np.linalg.inv(Xnot.T@Xnot)@Xnot.T\n",
    "Hnot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "53.478184817901784"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imat = np.eye(data.shape[0]) # Matrix I\n",
    "Xreg_t = (imat-Hnot)@Xreg # X1'\n",
    "target_t = (imat-Hnot)@target # y'\n",
    "\n",
    "model_partial= Lasso(alpha=0.01)\n",
    "model_partial.fit(Xreg_t, target_t)\n",
    "target_pred = model_partial.predict(Xreg_t)\n",
    "#Show the RMSE\n",
    "mean_squared_error(target_t, target_pred, squared=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a fair comparison we finish by building an ordinary linear model without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "53.47607314274362"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_OLS = LinearRegression()\n",
    "model_OLS.fit(data, target)\n",
    "target_pred = model_OLS.predict(data)\n",
    "#Show the RMSE\n",
    "mean_squared_error(target, target_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the three models in a table, the linear model without regularization is the one with lowest RMSE, followed very closely by the model with partial regularization. The standard LASSO model comes in third place.\n",
    "\n",
    "| Model | RMSE |\n",
    "|------|------|\n",
    "|   OLS  | 53.476 |\n",
    "| Partial LASSO | 53.478 |\n",
    "| LASSO | 53.563 |\n",
    "\n",
    "It needs to be said, this tutorial was just to show how to perform partial regularization. If we were trying to build a good model to predict the target value there would be other steps before getting into building the model.\n",
    "\n",
    "**Disclaimer:** We use the term partial regularization to express regularization for a subset of variables only in this tutorial. Probably there exists a standard term that has the same meaning."
   ]
  }
 ]
}